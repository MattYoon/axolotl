{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/dongkeun/.conda/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/nas/dongkeun/.conda/envs/torch/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 13626.72it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DKYoon/yanolja-hermes_en-orca-wiki-alpaca-lima')\n",
    "model = AutoPeftModelForCausalLM.from_pretrained('DKYoon/yanolja-hermes_en-orca-wiki-alpaca-lima', torch_dtype=torch.bfloat16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(x):\n",
    "    x = x.strip()\n",
    "    messages = [{\"role\": \"user\", \"content\": x},]\n",
    "    encodeds = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer(encodeds, return_tensors='pt')['input_ids'].to(DEVICE)\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=256, do_sample=False, eos_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|> \n",
      "<|im_start|>user\n",
      "하늘은 왜 파란색이야?<|im_end|> \n",
      "<|im_start|>assistant\n",
      "하늘이 파란색인 이유는 대기 중의 산소 분자와 질소 분자가 파란색의 빛을 흡수하고, 다른 색의 빛을 산란시키기 때문입니다. 파란색은 파장이 짧은 빛으로, 산소 분자와 질소 분자가 흡수하기 어려운 색입니다. 따라서 파란색 빛이 대기 중을 통과하면서 산란되어 하늘이 파란색으로 보입니다.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "infer(\"하늘은 왜 파란색이야?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
